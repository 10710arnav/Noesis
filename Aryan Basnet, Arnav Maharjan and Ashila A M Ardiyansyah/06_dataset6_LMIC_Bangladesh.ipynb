{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10710arnav/Noesis/blob/main/Aryan%20Basnet%2C%20Arnav%20Maharjan%20and%20Ashila%20A%20M%20Ardiyansyah/06_dataset6_LMIC_Bangladesh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NOTE ON RUNTIME AND OUTPUTS**\n",
        "\n",
        "# Google Colab may have disconnected or reset the runtime during long training sessions, crashes, or memory interruptions. When this occurred, some previously displayed outputs in the notebook were no longer visible. However, all results remained saved and logged correctly. Each model’s complete metrics and metadata were stored as JSON files in my Google Drive folder:\n",
        "\n",
        "# [https://drive.google.com/drive/folders/1ejlJaZhHEBm-1khLBJ--mbG2pg5TZHoJ?usp=sharing](https://drive.google.com/drive/folders/1ejlJaZhHEBm-1khLBJ--mbG2pg5TZHoJ?usp=sharing)\n",
        "\n",
        "# These JSON files contain the full and reliable outputs for all models across all datasets, even if certain notebook outputs were lost due to runtime resets."
      ],
      "metadata": {
        "id": "pMDWja913TWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================\n",
        "# SETUP: Freeze all package versions\n",
        "# ==============================\n",
        "Ensure reproducibility by installing the exact versions of packages used in these notebooks. This includes pre-installed packages in Colab.\n",
        "\n",
        "The packages and versions used are:\n",
        "\n",
        "- numpy==1.25.2\n",
        "- pandas==2.1.1\n",
        "- matplotlib==3.8.0\n",
        "- seaborn==0.12.2\n",
        "- scikit-learn==1.3.2\n",
        "- tensorflow==2.15.0\n",
        "- keras==2.15.0\n",
        "- scipy==1.11.2\n",
        "- opencv-python==4.9.0.73\n",
        "- Pillow==10.0.1\n",
        "- h5py==3.9.0\n",
        "- google-colab==2.0.0"
      ],
      "metadata": {
        "id": "1lx8AGnL3HuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/mydrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSQM0ZM0hXK8",
        "outputId": "7c63e965-940f-4ac4-ca37-484d1a62b995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/mydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Start from root of your mounted drive\n",
        "root = \"/content/mydrive/MyDrive/\"\n",
        "\n",
        "# List top-level folders in your Drive\n",
        "print(\"Top-level folders in MyDrive:\")\n",
        "print(os.listdir(root))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yfQgLrAwFNJ",
        "outputId": "8f15ae6a-0357-4b9f-9883-fef2547ba78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-level folders in MyDrive:\n",
            "['Getting started.pdf', 'Copy of Retro Internet Aesthetic Interface Theme for Marketing by Slidesgo.gslides', 'THE RESCUE - [NARRATIVE WRITING].gdoc', 'POEM-.gdoc', 'Chest-X-Ray Epic Hospital Chittagong, Bangladesh pneumonia', 'A Primary Chest X-ray Dataset of Normal and Pneumo', 'A Primary Chest X-ray Dataset of Normal and Pneumonia.zip', 'Colab Notebooks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Main folder\n",
        "dataset_root = \"/content/mydrive/MyDrive/Chest-X-Ray Epic Hospital Chittagong, Bangladesh pneumonia\"\n",
        "\n",
        "# Expected splits\n",
        "splits = [\"Training\", \"Testing\"]\n",
        "\n",
        "for split in splits:\n",
        "    split_path = os.path.join(dataset_root, split)\n",
        "\n",
        "    if not os.path.exists(split_path):\n",
        "        print(f\"[ERROR] Split folder not found: {split_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n--- {split.upper()} ---\")\n",
        "\n",
        "    # List classes in this split\n",
        "    for cls in os.listdir(split_path):\n",
        "        cls_path = os.path.join(split_path, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            images = [f for f in os.listdir(cls_path) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "            print(f\"{cls.lower()}: {len(images)} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxdzLsFzwMWc",
        "outputId": "0e3e6df5-245d-42cf-fa94-a71d1ecdfc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TRAINING ---\n",
            "pneumonia: 1050 images\n",
            "normal: 1063 images\n",
            "\n",
            "--- TESTING ---\n",
            "pneumonia: 256 images\n",
            "normal: 257 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CHEST X-RAY CLASSIFICATION - BANGLADESH DATASET\n",
        "# Using Pneumonia Dataset from Epic Hospital, Chittagong, Bangladesh\n",
        "# ==========================================\n",
        "\n",
        "# -------------------------------\n",
        "# IMPORT LIBRARIES\n",
        "# -------------------------------\n",
        "# Core utilities for file handling, timing, memory cleanup, JSON output, and warnings\n",
        "# Numpy for arrays, TensorFlow/Keras for deep learning\n",
        "# Sklearn for evaluation metrics and splitting data\n",
        "import os, zipfile, shutil, time, gc, json, warnings\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "warnings.filterwarnings('ignore')  # suppress unnecessary warnings\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 0: PATHS & DATASET LOCATION\n",
        "# -------------------------------\n",
        "# Mount Google Drive to access dataset and save results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Root folder containing the original dataset\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/Chest-X-Ray Epic Hospital Chittagong, Bangladesh pneumonia\"\n",
        "\n",
        "# Original folders already split into Training and Testing\n",
        "train_orig = os.path.join(DATASET_ROOT, \"Training\")\n",
        "test_orig  = os.path.join(DATASET_ROOT, \"Testing\")\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 1: METADATA\n",
        "# -------------------------------\n",
        "# Basic info for reference and saving results\n",
        "DATASET_NAME = \"Bangladesh_chest_xray\"\n",
        "COUNTRY_INCOME_LEVEL = \"LMIC\"\n",
        "NUM_CLASSES = 2\n",
        "CLASS_NAMES = ['Normal', 'Pneumonia']\n",
        "\n",
        "print(f\"Dataset: {DATASET_NAME}, Income Level: {COUNTRY_INCOME_LEVEL}, Classes: {CLASS_NAMES}\")\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 2: SPLIT DATA (optional)\n",
        "# -------------------------------\n",
        "# Create standardized folder structure for train/val/test\n",
        "base_dir = \"/content/Bangladesh_split\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "val_dir   = os.path.join(base_dir, \"val\")\n",
        "test_dir  = os.path.join(base_dir, \"test\")\n",
        "\n",
        "for d in [train_dir, val_dir, test_dir]:\n",
        "    for c in CLASS_NAMES:\n",
        "        os.makedirs(os.path.join(d, c), exist_ok=True)\n",
        "\n",
        "# Copy original training and testing images into new standardized structure\n",
        "for cls in CLASS_NAMES:\n",
        "    cls_folder = \"normal\" if cls==\"Normal\" else \"pneumonia\"\n",
        "    for f in os.listdir(os.path.join(train_orig, cls_folder)):\n",
        "        shutil.copy(os.path.join(train_orig, cls_folder, f), os.path.join(train_dir, cls))\n",
        "    for f in os.listdir(os.path.join(test_orig, cls_folder)):\n",
        "        shutil.copy(os.path.join(test_orig, cls_folder, f), os.path.join(test_dir, cls))\n",
        "\n",
        "# Split validation set from training data (20%) for hyperparameter tuning\n",
        "for cls in CLASS_NAMES:\n",
        "    files = os.listdir(os.path.join(train_dir, cls))\n",
        "    train_files, val_files = train_test_split(files, test_size=0.2, random_state=42)\n",
        "    val_cls_dir = os.path.join(val_dir, cls)\n",
        "    for f in val_files:\n",
        "        shutil.move(os.path.join(train_dir, cls, f), os.path.join(val_cls_dir, f))\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 3: DATA AUGMENTATION\n",
        "# -------------------------------\n",
        "# Image preprocessing and augmentation to improve generalization\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 8  # small batch due to dataset size\n",
        "EPOCHS = 20\n",
        "\n",
        "# Training generator with augmentations: rotation, shift, shear, zoom, brightness, flip\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Validation/test generator only rescales pixels\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Generators create batches and feed data to the model\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True\n",
        ")\n",
        "validation_generator = val_test_datagen.flow_from_directory(\n",
        "    val_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 4: CLASS WEIGHTS\n",
        "# -------------------------------\n",
        "# Compute class weights to handle imbalance in Normal vs Pneumonia images\n",
        "y_train = train_generator.classes\n",
        "class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
        "print(f\"\\nClass weights: {class_weights}\")\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 5: MODEL DEFINITIONS\n",
        "# -------------------------------\n",
        "# Baseline CNN: simple small architecture for comparison\n",
        "def create_baseline_cnn(input_shape=(224,224,3), num_classes=2):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Transfer learning models: MobileNetV2, EfficientNetB0, ResNet50\n",
        "# Freeze 85% of layers to prevent overfitting on small dataset\n",
        "def create_transfer_model(base_model_name, input_shape=(224,224,3), num_classes=2):\n",
        "    if base_model_name == 'MobileNetV2':\n",
        "        base = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    elif base_model_name == 'EfficientNetB0':\n",
        "        base = tf.keras.applications.EfficientNetB0(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    elif base_model_name == 'ResNet50':\n",
        "        base = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {base_model_name}\")\n",
        "\n",
        "    base.trainable = True\n",
        "    freeze_until = int(len(base.layers) * 0.85)\n",
        "    for layer in base.layers[:freeze_until]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = base(inputs, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 6: TRAINING FUNCTION\n",
        "# -------------------------------\n",
        "# Compile, train, and evaluate any given model, save results\n",
        "def train_and_evaluate(model, model_name):\n",
        "    print(f\"\\n{'='*50}\\nTraining {model_name}\\n{'='*50}\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-4),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Early stopping and LR reduction callbacks to avoid overfitting\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7)\n",
        "\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[early_stop, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "    training_time = (time.time() - start_time)/60  # convert to minutes\n",
        "\n",
        "    # Evaluate on test set\n",
        "    predictions = model.predict(test_generator)\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "    y_true = test_generator.classes\n",
        "\n",
        "    f1_per_class = f1_score(y_true, y_pred, average=None).tolist()\n",
        "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Save all results in JSON\n",
        "    results = {\n",
        "        'dataset_name': DATASET_NAME,\n",
        "        'country_income': COUNTRY_INCOME_LEVEL,\n",
        "        'model_name': model_name,\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'class_names': CLASS_NAMES,\n",
        "        'f1_per_class': f1_per_class,\n",
        "        'f1_weighted': float(f1_weighted),\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'training_time_minutes': float(training_time),\n",
        "        'num_images_train': train_generator.samples,\n",
        "        'num_images_val': validation_generator.samples,\n",
        "        'num_images_test': test_generator.samples,\n",
        "        'num_parameters': int(model.count_params()),\n",
        "        'epochs_trained': len(history.history['loss'])\n",
        "    }\n",
        "\n",
        "    os.makedirs('/content/drive/MyDrive/xray_research_results', exist_ok=True)\n",
        "    filename = f'/content/drive/MyDrive/xray_research_results/{DATASET_NAME}_{model_name}_results.json'\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"{model_name} Weighted F1: {f1_weighted:.4f}, F1 per class: {f1_per_class}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "    print(f\"Training time: {training_time:.2f} min, Params: {model.count_params():,}\")\n",
        "\n",
        "    # Cleanup to free GPU memory\n",
        "    del model\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 7: TRAIN ALL MODELS\n",
        "# -------------------------------\n",
        "all_results = []\n",
        "for model_name in ['BaselineCNN', 'MobileNetV2', 'EfficientNetB0', 'ResNet50']:\n",
        "    if model_name == 'BaselineCNN':\n",
        "        model = create_baseline_cnn(num_classes=NUM_CLASSES)\n",
        "    else:\n",
        "        model = create_transfer_model(model_name, num_classes=NUM_CLASSES)\n",
        "    results = train_and_evaluate(model, model_name)\n",
        "    all_results.append(results)\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 8: SUMMARY\n",
        "# -------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL TRAINING COMPLETE\")\n",
        "for r in all_results:\n",
        "    print(f\"{r['model_name']}: Weighted F1: {r['f1_weighted']:.4f}, Epochs: {r['epochs_trained']}, Params: {r['num_parameters']:,}\")\n",
        "print(f\"\\nAll results saved to: /content/drive/MyDrive/xray_research_results/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrkw0-wHwkry",
        "outputId": "0453b89b-3011-4799-824c-43bcf381cf90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset: Bangladesh_chest_xray, Income Level: LMIC, Classes: ['Normal', 'Pneumonia']\n",
            "Found 1690 images belonging to 2 classes.\n",
            "Found 423 images belonging to 2 classes.\n",
            "Found 513 images belonging to 2 classes.\n",
            "\n",
            "Class weights: {0: np.float64(0.9941176470588236), 1: np.float64(1.005952380952381)}\n",
            "\n",
            "==================================================\n",
            "Training BaselineCNN\n",
            "==================================================\n",
            "Epoch 1/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 1s/step - accuracy: 0.6739 - loss: 0.5878 - val_accuracy: 0.8865 - val_loss: 0.2909 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 1s/step - accuracy: 0.8586 - loss: 0.3689 - val_accuracy: 0.8960 - val_loss: 0.2516 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 1s/step - accuracy: 0.8490 - loss: 0.3728 - val_accuracy: 0.8889 - val_loss: 0.2481 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 1s/step - accuracy: 0.8801 - loss: 0.2993 - val_accuracy: 0.8889 - val_loss: 0.2363 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 1s/step - accuracy: 0.8804 - loss: 0.2878 - val_accuracy: 0.8913 - val_loss: 0.2445 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 1s/step - accuracy: 0.8796 - loss: 0.3053 - val_accuracy: 0.8889 - val_loss: 0.2391 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 1s/step - accuracy: 0.8836 - loss: 0.2794 - val_accuracy: 0.8913 - val_loss: 0.2152 - learning_rate: 5.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 1s/step - accuracy: 0.8953 - loss: 0.2661 - val_accuracy: 0.9007 - val_loss: 0.2201 - learning_rate: 5.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 1s/step - accuracy: 0.8879 - loss: 0.2855 - val_accuracy: 0.8983 - val_loss: 0.2069 - learning_rate: 5.0000e-05\n",
            "Epoch 10/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 1s/step - accuracy: 0.9005 - loss: 0.2651 - val_accuracy: 0.9054 - val_loss: 0.2093 - learning_rate: 5.0000e-05\n",
            "Epoch 11/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 1s/step - accuracy: 0.8829 - loss: 0.2649 - val_accuracy: 0.8913 - val_loss: 0.2099 - learning_rate: 5.0000e-05\n",
            "Epoch 12/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 1s/step - accuracy: 0.8982 - loss: 0.2436 - val_accuracy: 0.8983 - val_loss: 0.1993 - learning_rate: 2.5000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 1s/step - accuracy: 0.8885 - loss: 0.2589 - val_accuracy: 0.8983 - val_loss: 0.1951 - learning_rate: 2.5000e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 1s/step - accuracy: 0.8897 - loss: 0.2636 - val_accuracy: 0.9031 - val_loss: 0.1920 - learning_rate: 2.5000e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 1s/step - accuracy: 0.9049 - loss: 0.2316 - val_accuracy: 0.8983 - val_loss: 0.2002 - learning_rate: 2.5000e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 1s/step - accuracy: 0.9021 - loss: 0.2427 - val_accuracy: 0.9031 - val_loss: 0.1927 - learning_rate: 2.5000e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 1s/step - accuracy: 0.8989 - loss: 0.2392 - val_accuracy: 0.9031 - val_loss: 0.2009 - learning_rate: 1.2500e-05\n",
            "Epoch 18/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 1s/step - accuracy: 0.9078 - loss: 0.2317 - val_accuracy: 0.9054 - val_loss: 0.1942 - learning_rate: 1.2500e-05\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 383ms/step\n",
            "BaselineCNN Weighted F1: 0.9001, F1 per class: [0.8935281837160751, 0.906764168190128]\n",
            "Confusion Matrix:\n",
            "[[214  43]\n",
            " [  8 248]]\n",
            "Training time: 78.39 min, Params: 11,132,098\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "==================================================\n",
            "Training MobileNetV2\n",
            "==================================================\n",
            "Epoch 1/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 766ms/step - accuracy: 0.7970 - loss: 0.4364 - val_accuracy: 0.7825 - val_loss: 0.6563 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 758ms/step - accuracy: 0.8899 - loss: 0.2902 - val_accuracy: 0.8960 - val_loss: 0.3173 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 759ms/step - accuracy: 0.8658 - loss: 0.2673 - val_accuracy: 0.9031 - val_loss: 0.3305 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 756ms/step - accuracy: 0.8978 - loss: 0.2137 - val_accuracy: 0.8913 - val_loss: 0.3433 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 770ms/step - accuracy: 0.9096 - loss: 0.1927 - val_accuracy: 0.8771 - val_loss: 0.3272 - learning_rate: 5.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 763ms/step - accuracy: 0.9083 - loss: 0.1969 - val_accuracy: 0.9007 - val_loss: 0.2216 - learning_rate: 5.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 772ms/step - accuracy: 0.9022 - loss: 0.2111 - val_accuracy: 0.9031 - val_loss: 0.2069 - learning_rate: 5.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 764ms/step - accuracy: 0.9002 - loss: 0.2062 - val_accuracy: 0.9102 - val_loss: 0.1836 - learning_rate: 5.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 777ms/step - accuracy: 0.9108 - loss: 0.1713 - val_accuracy: 0.9078 - val_loss: 0.1819 - learning_rate: 5.0000e-05\n",
            "Epoch 10/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 753ms/step - accuracy: 0.9030 - loss: 0.2234 - val_accuracy: 0.9031 - val_loss: 0.1957 - learning_rate: 5.0000e-05\n",
            "Epoch 11/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 770ms/step - accuracy: 0.8986 - loss: 0.1950 - val_accuracy: 0.9007 - val_loss: 0.2197 - learning_rate: 5.0000e-05\n",
            "Epoch 12/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 759ms/step - accuracy: 0.8871 - loss: 0.2043 - val_accuracy: 0.9054 - val_loss: 0.2104 - learning_rate: 2.5000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 775ms/step - accuracy: 0.9122 - loss: 0.1824 - val_accuracy: 0.9007 - val_loss: 0.2091 - learning_rate: 2.5000e-05\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 489ms/step\n",
            "MobileNetV2 Weighted F1: 0.9276, F1 per class: [0.9230769230769231, 0.9321100917431193]\n",
            "Confusion Matrix:\n",
            "[[222  35]\n",
            " [  2 254]]\n",
            "Training time: 36.00 min, Params: 2,422,210\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "==================================================\n",
            "Training EfficientNetB0\n",
            "==================================================\n",
            "Epoch 1/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 1s/step - accuracy: 0.4842 - loss: 0.7352 - val_accuracy: 0.4965 - val_loss: 0.7126 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.4903 - loss: 0.7126 - val_accuracy: 0.4965 - val_loss: 0.6929 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 1s/step - accuracy: 0.4801 - loss: 0.7107 - val_accuracy: 0.5035 - val_loss: 0.6918 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 1s/step - accuracy: 0.5146 - loss: 0.6982 - val_accuracy: 0.4965 - val_loss: 0.6935 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 1s/step - accuracy: 0.4941 - loss: 0.6971 - val_accuracy: 0.7329 - val_loss: 0.6920 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 1s/step - accuracy: 0.4846 - loss: 0.7009 - val_accuracy: 0.4965 - val_loss: 0.6923 - learning_rate: 5.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 1s/step - accuracy: 0.5030 - loss: 0.6988 - val_accuracy: 0.4965 - val_loss: 0.6925 - learning_rate: 5.0000e-05\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 733ms/step\n",
            "EfficientNetB0 Weighted F1: 0.3344, F1 per class: [0.6675324675324675, 0.0]\n",
            "Confusion Matrix:\n",
            "[[257   0]\n",
            " [256   0]]\n",
            "Training time: 27.50 min, Params: 4,213,797\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "==================================================\n",
            "Training ResNet50\n",
            "==================================================\n",
            "Epoch 1/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 3s/step - accuracy: 0.6747 - loss: 0.5916 - val_accuracy: 0.8629 - val_loss: 0.3553 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 3s/step - accuracy: 0.8194 - loss: 0.4260 - val_accuracy: 0.6998 - val_loss: 0.8964 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 3s/step - accuracy: 0.8241 - loss: 0.4540 - val_accuracy: 0.6690 - val_loss: 0.6370 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m577s\u001b[0m 3s/step - accuracy: 0.8540 - loss: 0.3678 - val_accuracy: 0.8345 - val_loss: 0.3746 - learning_rate: 5.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 3s/step - accuracy: 0.8665 - loss: 0.3263 - val_accuracy: 0.8014 - val_loss: 0.3795 - learning_rate: 5.0000e-05\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step\n",
            "ResNet50 Weighted F1: 0.8680, F1 per class: [0.8546637744034707, 0.8814159292035398]\n",
            "Confusion Matrix:\n",
            "[[197  60]\n",
            " [  7 249]]\n",
            "Training time: 50.52 min, Params: 23,850,242\n",
            "\n",
            "============================================================\n",
            "ALL TRAINING COMPLETE\n",
            "BaselineCNN: Weighted F1: 0.9001, Epochs: 18, Params: 11,132,098\n",
            "MobileNetV2: Weighted F1: 0.9276, Epochs: 13, Params: 2,422,210\n",
            "EfficientNetB0: Weighted F1: 0.3344, Epochs: 7, Params: 4,213,797\n",
            "ResNet50: Weighted F1: 0.8680, Epochs: 5, Params: 23,850,242\n",
            "\n",
            "All results saved to: /content/drive/MyDrive/xray_research_results/\n"
          ]
        }
      ]
    }
  ]
}