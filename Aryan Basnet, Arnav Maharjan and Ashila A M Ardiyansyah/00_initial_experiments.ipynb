{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNieCiM/v4zvLmIKw83XSgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10710arnav/Noesis/blob/main/Aryan%20Basnet%2C%20Arnav%20Maharjan%20and%20Ashila%20A%20M%20Ardiyansyah/00_initial_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE: This Colab notebook is a **test run**.\n",
        "# The RSUA dataset is relatively small and this script is used primarily to demonstrate the workflow for training different models (Baseline CNN, MobileNetV2, EfficientNetB0, ResNet50) on a dataset. This mirrors the approach we took for the six main datasets in our research. Outcomes here are not intended for publication-level results; the purpose is to validate the pipeline and ensure proper handling of each model type.\n"
      ],
      "metadata": {
        "id": "NEpR5JAFB4Co"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NOTE ON RUNTIME AND OUTPUTS**\n",
        "\n",
        "# Google Colab may have disconnected or reset the runtime during long training sessions, crashes, or memory interruptions. When this occurred, some previously displayed outputs in the notebook were no longer visible. However, all results remained saved and logged correctly. Each model’s complete metrics and metadata were stored as JSON files in my Google Drive folder:\n",
        "\n",
        "# [https://drive.google.com/drive/folders/1ejlJaZhHEBm-1khLBJ--mbG2pg5TZHoJ?usp=sharing](https://drive.google.com/drive/folders/1ejlJaZhHEBm-1khLBJ--mbG2pg5TZHoJ?usp=sharing)\n",
        "\n",
        "# These JSON files contain the full and reliable outputs for all models across all datasets, even if certain notebook outputs were lost due to runtime resets."
      ],
      "metadata": {
        "id": "ZEI6MHYZ_zrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================\n",
        "# SETUP: Freeze all package versions\n",
        "# ==============================\n",
        "Ensure reproducibility by installing the exact versions of packages used in these notebooks. This includes pre-installed packages in Colab.\n",
        "\n",
        "The packages and versions used are:\n",
        "\n",
        "- numpy==1.25.2\n",
        "- pandas==2.1.1\n",
        "- matplotlib==3.8.0\n",
        "- seaborn==0.12.2\n",
        "- scikit-learn==1.3.2\n",
        "- tensorflow==2.15.0\n",
        "- keras==2.15.0\n",
        "- scipy==1.11.2\n",
        "- opencv-python==4.9.0.73\n",
        "- Pillow==10.0.1\n",
        "- h5py==3.9.0\n",
        "- google-colab==2.0.0"
      ],
      "metadata": {
        "id": "yiadpGi98q72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, os, shutil, time, gc, json, warnings\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==========================\n",
        "# STEP 0: PATHS AND EXTRACTION\n",
        "# ==========================\n",
        "zip_path = '/content/Data Chest X-Ray RSUA (Annotated)-20230618T030427Z-001.zip'\n",
        "extract_path = '/content/RSUA_dataset'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Dataset extracted to: {extract_path}\")\n",
        "else:\n",
        "    print(f\"Dataset already extracted at: {extract_path}\")\n",
        "\n",
        "DATASET_ROOT = \"/content/RSUA_dataset/Data Chest X-Ray RSUA (Annotated)\"\n",
        "normal_dir = os.path.join(DATASET_ROOT, \"Non_Covid\", \"images\")\n",
        "pneumonia_dir = os.path.join(DATASET_ROOT, \"Non_Covid_Pneumonia\", \"images\")\n",
        "\n",
        "# ==========================\n",
        "# STEP 1: METADATA\n",
        "# ==========================\n",
        "DATASET_NAME = \"RSUA_chest_xray\"\n",
        "COUNTRY_INCOME_LEVEL = \"LMIC\"\n",
        "NUM_CLASSES = 2\n",
        "CLASS_NAMES = ['Normal', 'Pneumonia']\n",
        "\n",
        "print(f\"Dataset: {DATASET_NAME}, Income Level: {COUNTRY_INCOME_LEVEL}, Classes: {CLASS_NAMES}\")\n",
        "\n",
        "# ==========================\n",
        "# STEP 2: MOUNT DRIVE\n",
        "# ==========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.makedirs('/content/drive/MyDrive/xray_research_results', exist_ok=True)\n",
        "\n",
        "# ==========================\n",
        "# STEP 3: SPLIT FUNCTION\n",
        "# ==========================\n",
        "base_dir = \"/content/RSUA_split\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "val_dir = os.path.join(base_dir, \"val\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "for d in [train_dir, val_dir, test_dir]:\n",
        "    for c in CLASS_NAMES:\n",
        "        os.makedirs(os.path.join(d, c), exist_ok=True)\n",
        "\n",
        "def split_copy(src_dir, class_name):\n",
        "    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp')\n",
        "    all_files = []\n",
        "    for root, dirs, files in os.walk(src_dir):\n",
        "        for f in files:\n",
        "            if f.lower().endswith(image_extensions):\n",
        "                all_files.append(os.path.join(root, f))\n",
        "\n",
        "    train_files, temp_files = train_test_split(all_files, test_size=0.3, random_state=42)\n",
        "    val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n",
        "\n",
        "    for fpath in train_files:\n",
        "        shutil.copy(fpath, os.path.join(train_dir, class_name))\n",
        "    for fpath in val_files:\n",
        "        shutil.copy(fpath, os.path.join(val_dir, class_name))\n",
        "    for fpath in test_files:\n",
        "        shutil.copy(fpath, os.path.join(test_dir, class_name))\n",
        "\n",
        "split_copy(normal_dir, 'Normal')\n",
        "split_copy(pneumonia_dir, 'Pneumonia')\n",
        "\n",
        "# ==========================\n",
        "# STEP 4: DATA AUGMENTATION\n",
        "# ==========================\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 20\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True\n",
        ")\n",
        "validation_generator = val_test_datagen.flow_from_directory(\n",
        "    val_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "\n",
        "# ==========================\n",
        "# STEP 5: CLASS WEIGHTS\n",
        "# ==========================\n",
        "y_train = train_generator.classes\n",
        "class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
        "print(f\"\\nClass weights: {class_weights}\")\n",
        "\n",
        "# ==========================\n",
        "# STEP 6: MODEL DEFINITIONS\n",
        "# ==========================\n",
        "def create_baseline_cnn(input_shape=(224,224,3), num_classes=2):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_transfer_model(base_model_name, input_shape=(224,224,3), num_classes=2):\n",
        "    if base_model_name == 'MobileNetV2':\n",
        "        base = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    elif base_model_name == 'EfficientNetB0':\n",
        "        base = tf.keras.applications.EfficientNetB0(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    elif base_model_name == 'ResNet50':\n",
        "        base = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {base_model_name}\")\n",
        "\n",
        "    base.trainable = True\n",
        "    freeze_until = int(len(base.layers) * 0.85)\n",
        "    for layer in base.layers[:freeze_until]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = base(inputs, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "# ==========================\n",
        "# STEP 7: TRAINING FUNCTION\n",
        "# ==========================\n",
        "def train_and_evaluate(model, model_name):\n",
        "    print(f\"\\n{'='*50}\\nTraining {model_name}\\n{'='*50}\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-4),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7)\n",
        "\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[early_stop, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "    training_time = (time.time() - start_time)/60\n",
        "\n",
        "    predictions = model.predict(test_generator)\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "    y_true = test_generator.classes\n",
        "\n",
        "    f1_per_class = f1_score(y_true, y_pred, average=None).tolist()\n",
        "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    results = {\n",
        "        'dataset_name': DATASET_NAME,\n",
        "        'country_income': COUNTRY_INCOME_LEVEL,\n",
        "        'model_name': model_name,\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'class_names': CLASS_NAMES,\n",
        "        'f1_per_class': f1_per_class,\n",
        "        'f1_weighted': float(f1_weighted),\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'training_time_minutes': float(training_time),\n",
        "        'num_images_train': train_generator.samples,\n",
        "        'num_images_val': validation_generator.samples,\n",
        "        'num_images_test': test_generator.samples,\n",
        "        'num_parameters': int(model.count_params()),\n",
        "        'epochs_trained': len(history.history['loss'])\n",
        "    }\n",
        "\n",
        "    filename = f'/content/drive/MyDrive/xray_research_results/{DATASET_NAME}_{model_name}_results.json'\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"{model_name} Weighted F1: {f1_weighted:.4f}, F1 per class: {f1_per_class}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "    print(f\"Training time: {training_time:.2f} min, Params: {model.count_params():,}\")\n",
        "\n",
        "    del model\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    return results\n",
        "\n",
        "# ==========================\n",
        "# STEP 8: TRAIN ALL 4 MODELS\n",
        "# ==========================\n",
        "all_results = []\n",
        "for model_name in ['BaselineCNN', 'MobileNetV2', 'EfficientNetB0', 'ResNet50']:\n",
        "    if model_name == 'BaselineCNN':\n",
        "        model = create_baseline_cnn(num_classes=NUM_CLASSES)\n",
        "    else:\n",
        "        model = create_transfer_model(model_name, num_classes=NUM_CLASSES)\n",
        "    results = train_and_evaluate(model, model_name)\n",
        "    all_results.append(results)\n",
        "\n",
        "# ==========================\n",
        "# STEP 9: SUMMARY\n",
        "# ==========================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL TRAINING COMPLETE\")\n",
        "for r in all_results:\n",
        "    print(f\"{r['model_name']}: Weighted F1: {r['f1_weighted']:.4f}, Epochs: {r['epochs_trained']}, Params: {r['num_parameters']:,}\")\n",
        "print(f\"\\nAll results saved to: /content/drive/MyDrive/xray_research_results/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ9FJquRKn6P",
        "outputId": "77eaa056-ad26-429a-d24b-574743f6ead3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already extracted at: /content/RSUA_dataset\n",
            "Dataset: RSUA_chest_xray, Income Level: LMIC, Classes: ['Normal', 'Pneumonia']\n",
            "Mounted at /content/drive\n",
            "Found 98 images belonging to 2 classes.\n",
            "Found 54 images belonging to 2 classes.\n",
            "Found 54 images belonging to 2 classes.\n",
            "\n",
            "Class weights: {0: np.float64(1.2564102564102564), 1: np.float64(0.8305084745762712)}\n",
            "\n",
            "==================================================\n",
            "Training BaselineCNN\n",
            "==================================================\n",
            "Epoch 1/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.4394 - loss: 0.8327 - val_accuracy: 0.5741 - val_loss: 0.6833 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 993ms/step - accuracy: 0.5595 - loss: 0.7244 - val_accuracy: 0.4815 - val_loss: 0.6932 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 970ms/step - accuracy: 0.5513 - loss: 0.6893 - val_accuracy: 0.4444 - val_loss: 0.7046 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 974ms/step - accuracy: 0.5658 - loss: 0.6967 - val_accuracy: 0.4259 - val_loss: 0.7140 - learning_rate: 5.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 965ms/step - accuracy: 0.6403 - loss: 0.6525 - val_accuracy: 0.4815 - val_loss: 0.7096 - learning_rate: 5.0000e-05\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step\n",
            "BaselineCNN Weighted F1: 0.5774, F1 per class: [0.3225806451612903, 0.7272727272727273]\n",
            "Confusion Matrix:\n",
            "[[ 5 15]\n",
            " [ 6 28]]\n",
            "Training time: 1.28 min, Params: 11,132,098\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "\n",
            "==================================================\n",
            "Training MobileNetV2\n",
            "==================================================\n",
            "Epoch 1/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 844ms/step - accuracy: 0.4556 - loss: 1.1111 - val_accuracy: 0.4074 - val_loss: 1.3830 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 683ms/step - accuracy: 0.6504 - loss: 0.6635 - val_accuracy: 0.3889 - val_loss: 1.4781 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 539ms/step - accuracy: 0.6212 - loss: 0.7433 - val_accuracy: 0.3889 - val_loss: 1.1958 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 676ms/step - accuracy: 0.6361 - loss: 0.6013 - val_accuracy: 0.3889 - val_loss: 1.2528 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 552ms/step - accuracy: 0.6582 - loss: 0.5725 - val_accuracy: 0.4444 - val_loss: 1.0601 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 672ms/step - accuracy: 0.7033 - loss: 0.5602 - val_accuracy: 0.5000 - val_loss: 1.0070 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 617ms/step - accuracy: 0.7685 - loss: 0.5219 - val_accuracy: 0.5556 - val_loss: 0.7610 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 576ms/step - accuracy: 0.6972 - loss: 0.5702 - val_accuracy: 0.4815 - val_loss: 0.7549 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 671ms/step - accuracy: 0.7646 - loss: 0.5332 - val_accuracy: 0.4815 - val_loss: 0.7679 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 552ms/step - accuracy: 0.8191 - loss: 0.4432 - val_accuracy: 0.5000 - val_loss: 0.7586 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 689ms/step - accuracy: 0.8547 - loss: 0.4479 - val_accuracy: 0.5185 - val_loss: 0.7326 - learning_rate: 5.0000e-05\n",
            "Epoch 12/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 541ms/step - accuracy: 0.7242 - loss: 0.4810 - val_accuracy: 0.5370 - val_loss: 0.6946 - learning_rate: 5.0000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 698ms/step - accuracy: 0.8273 - loss: 0.4078 - val_accuracy: 0.5556 - val_loss: 0.6916 - learning_rate: 5.0000e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 647ms/step - accuracy: 0.7431 - loss: 0.5780 - val_accuracy: 0.5741 - val_loss: 0.6651 - learning_rate: 5.0000e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 580ms/step - accuracy: 0.7111 - loss: 0.4805 - val_accuracy: 0.5741 - val_loss: 0.6842 - learning_rate: 5.0000e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 694ms/step - accuracy: 0.8299 - loss: 0.3916 - val_accuracy: 0.5926 - val_loss: 0.6370 - learning_rate: 5.0000e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 554ms/step - accuracy: 0.8146 - loss: 0.4322 - val_accuracy: 0.5556 - val_loss: 0.6543 - learning_rate: 5.0000e-05\n",
            "Epoch 18/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 698ms/step - accuracy: 0.8767 - loss: 0.3448 - val_accuracy: 0.5556 - val_loss: 0.6537 - learning_rate: 5.0000e-05\n",
            "Epoch 19/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 554ms/step - accuracy: 0.7875 - loss: 0.4143 - val_accuracy: 0.5556 - val_loss: 0.6393 - learning_rate: 2.5000e-05\n",
            "Epoch 20/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 689ms/step - accuracy: 0.8326 - loss: 0.3904 - val_accuracy: 0.5556 - val_loss: 0.6288 - learning_rate: 2.5000e-05\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 470ms/step\n",
            "MobileNetV2 Weighted F1: 0.6537, F1 per class: [0.5777777777777777, 0.6984126984126984]\n",
            "Confusion Matrix:\n",
            "[[13  7]\n",
            " [12 22]]\n",
            "Training time: 2.93 min, Params: 2,422,210\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "\n",
            "==================================================\n",
            "Training EfficientNetB0\n",
            "==================================================\n",
            "Epoch 1/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.4737 - loss: 0.7625 - val_accuracy: 0.3704 - val_loss: 0.6971 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 989ms/step - accuracy: 0.4163 - loss: 0.7239 - val_accuracy: 0.6296 - val_loss: 0.6755 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.5259 - loss: 0.7346 - val_accuracy: 0.6296 - val_loss: 0.6698 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 979ms/step - accuracy: 0.5681 - loss: 0.6697 - val_accuracy: 0.6296 - val_loss: 0.6652 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 989ms/step - accuracy: 0.3276 - loss: 0.7859 - val_accuracy: 0.6296 - val_loss: 0.6611 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 956ms/step - accuracy: 0.5501 - loss: 0.6710 - val_accuracy: 0.6296 - val_loss: 0.6600 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.5246 - loss: 0.7424 - val_accuracy: 0.6296 - val_loss: 0.6598 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.5599 - loss: 0.7073 - val_accuracy: 0.6296 - val_loss: 0.6594 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 895ms/step - accuracy: 0.5956 - loss: 0.6754 - val_accuracy: 0.6296 - val_loss: 0.6591 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 860ms/step - accuracy: 0.6040 - loss: 0.6729 - val_accuracy: 0.6296 - val_loss: 0.6597 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 924ms/step - accuracy: 0.5509 - loss: 0.7047 - val_accuracy: 0.6296 - val_loss: 0.6617 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 964ms/step - accuracy: 0.5218 - loss: 0.6964 - val_accuracy: 0.6296 - val_loss: 0.6617 - learning_rate: 5.0000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 971ms/step - accuracy: 0.4503 - loss: 0.7878 - val_accuracy: 0.6296 - val_loss: 0.6626 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78aecdbba980> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 993ms/step\n",
            "EfficientNetB0 Weighted F1: 0.4865, F1 per class: [0.0, 0.7727272727272727]\n",
            "Confusion Matrix:\n",
            "[[ 0 20]\n",
            " [ 0 34]]\n",
            "Training time: 3.07 min, Params: 4,213,797\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
            "\n",
            "==================================================\n",
            "Training ResNet50\n",
            "==================================================\n",
            "Epoch 1/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - accuracy: 0.3735 - loss: 1.0599 - val_accuracy: 0.6296 - val_loss: 0.6605 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.5531 - loss: 0.7701 - val_accuracy: 0.6296 - val_loss: 0.6642 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.4620 - loss: 0.7267 - val_accuracy: 0.6296 - val_loss: 0.6601 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3s/step - accuracy: 0.5554 - loss: 0.7164 - val_accuracy: 0.6296 - val_loss: 0.6563 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.5071 - loss: 0.7014 - val_accuracy: 0.6481 - val_loss: 0.6526 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.5229 - loss: 0.6989 - val_accuracy: 0.6667 - val_loss: 0.6627 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - accuracy: 0.5582 - loss: 0.7031 - val_accuracy: 0.5926 - val_loss: 0.6659 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.5433 - loss: 0.6922 - val_accuracy: 0.6481 - val_loss: 0.6670 - learning_rate: 5.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.5785 - loss: 0.6923 - val_accuracy: 0.5741 - val_loss: 0.6793 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78aeccbc9c60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step\n",
            "ResNet50 Weighted F1: 0.4865, F1 per class: [0.0, 0.7727272727272727]\n",
            "Confusion Matrix:\n",
            "[[ 0 20]\n",
            " [ 0 34]]\n",
            "Training time: 5.12 min, Params: 23,850,242\n",
            "\n",
            "============================================================\n",
            "ALL TRAINING COMPLETE\n",
            "BaselineCNN: Weighted F1: 0.5774, Epochs: 5, Params: 11,132,098\n",
            "MobileNetV2: Weighted F1: 0.6537, Epochs: 20, Params: 2,422,210\n",
            "EfficientNetB0: Weighted F1: 0.4865, Epochs: 13, Params: 4,213,797\n",
            "ResNet50: Weighted F1: 0.4865, Epochs: 9, Params: 23,850,242\n",
            "\n",
            "All results saved to: /content/drive/MyDrive/xray_research_results/\n"
          ]
        }
      ]
    }
  ]
}